{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pulkit Bhatia\n",
    "102003306\n",
    "3CO12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from collections import Counter\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('/Users/pulkitbhatia/coding/predanal/Creditcard_data.csv')\n",
    "\n",
    "x=df.drop('Class',axis=1)\n",
    "y=df['Class']\n",
    "\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "x_ros, y_ros = ros.fit_resample(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(772, 30) (772,)\n",
      "(1526, 30) (1526,)\n",
      "(1526, 31)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape,y.shape)\n",
    "print(x_ros.shape,y_ros.shape)\n",
    "\n",
    "balanced_df=pd.concat([x_ros, y_ros], axis=1)\n",
    "print(balanced_df.shape)\n",
    "\n",
    "balanced_df.to_csv('Balanced_data.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(384, 31)\n"
     ]
    }
   ],
   "source": [
    "sample_size=int((pow(1.96,2)*0.5*0.5)/0.0025)\n",
    "sample_size\n",
    "\n",
    "random_sample = balanced_df.sample(n=sample_size, random_state=0)\n",
    "print(random_sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression accuracy: 0.8958333333333334\n",
      "Decision Tree accuracy: 0.9791666666666666\n",
      "Support Vector Classification accuracy: 0.6979166666666666\n",
      "KNeighborsClassifier accuracy: 0.9479166666666666\n",
      "Random Forest accuracy: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pulkitbhatia/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pulkitbhatia/opt/anaconda3/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Random_Sampling</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.895833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree</th>\n",
       "      <td>0.979167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC</th>\n",
       "      <td>0.697917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>0.947917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Random_Sampling\n",
       "Logistic Regression         0.895833\n",
       "Decision Tree               0.979167\n",
       "SVC                         0.697917\n",
       "KNN                         0.947917\n",
       "Random Forest               1.000000"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "x_random= random_sample.drop(columns=['Class'])\n",
    "y_random = random_sample['Class']\n",
    "x_random_train, x_random_test,y_random_train, y_random_test = train_test_split(x_random,y_random, random_state=132, \n",
    "                                   test_size=0.25, \n",
    "                                   shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "# Train and evaluate the models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Support Vector Classification\": SVC(),\n",
    "    \"KNeighborsClassifier\":KNeighborsClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier()  \n",
    "}\n",
    "ans=[]\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(x_random_train,y_random_train)\n",
    "    y_pred = model.predict(x_random_test)\n",
    "    temp = accuracy_score(y_random_test, y_pred)\n",
    "    print(f\"{name} accuracy: {temp}\")\n",
    "    ans.append(temp)\n",
    "ans_df=pd.DataFrame(ans,index=['Logistic Regression','Decision Tree','SVC','KNN','Random Forest'])\n",
    "ans_df.rename(columns={ ans_df.columns[0]: \"Random_Sampling\" }, inplace = True)\n",
    "ans_df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Systematic Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1526, 31)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Time        V1        V2        V3        V4        V5        V6  \\\n",
      "0        0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388   \n",
      "39      29  1.110880  0.168717  0.517144  1.325407 -0.191573  0.019504   \n",
      "78      50 -0.571521  1.071600  1.280110  0.542780  0.574439 -0.259359   \n",
      "117     76 -1.024576  0.522289  1.787699  0.202672 -1.140803 -0.137831   \n",
      "156     98 -0.646513  1.004199  1.616224 -0.099628 -0.122477 -0.671327   \n",
      "195    128  1.239495 -0.182609  0.155058 -0.928892 -0.746227 -1.235608   \n",
      "234    156  0.714401 -0.493905  1.269485  3.011494 -0.801216  0.929404   \n",
      "273    194 -1.131517  1.016399  0.735810  1.166614  0.790236 -1.187196   \n",
      "312    225  1.478773 -0.551089 -0.523152 -0.831153 -0.195413 -0.289193   \n",
      "351    259 -1.569485 -1.932133  1.249203 -4.434211  1.244282  0.402688   \n",
      "390    284 -0.942623  0.657318  1.191544  1.326497  0.976745 -0.832970   \n",
      "429    310 -0.728857 -0.020542  1.415026  1.233902 -0.240077  0.482240   \n",
      "468    344 -3.495984 -4.088420  2.024845 -0.740363  1.128135 -1.231702   \n",
      "507    375 -0.837689  0.777698  1.841252  3.056892  0.303627  0.615335   \n",
      "546    409  1.243758 -0.777764  1.016830 -0.250546 -1.268555  0.137280   \n",
      "585    438  1.287087 -0.392364  0.513360 -1.237322 -0.402556  0.474908   \n",
      "624    472  1.040781  0.109569  0.357987  1.118998 -0.105373 -0.056837   \n",
      "663    501 -1.377718  1.354530  0.543183 -1.427171 -0.124389 -0.524309   \n",
      "702    530  1.217261 -0.080646 -0.059293 -0.868862 -0.236628 -0.700159   \n",
      "741    557  1.173585  0.250650  0.839631  1.066309 -0.365090 -0.161887   \n",
      "780    472 -3.043541 -3.157307  1.088463  2.288644  1.359805 -1.064823   \n",
      "819    406 -2.312227  1.951992 -1.609851  3.997906 -0.522188 -1.426545   \n",
      "858    529 -2.000567 -2.495484  2.467149  1.140053  2.462010  0.594262   \n",
      "897    529 -2.000567 -2.495484  2.467149  1.140053  2.462010  0.594262   \n",
      "936      0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361   \n",
      "975    118  1.254914  0.350287  0.302488  0.693114 -0.371470 -1.070256   \n",
      "1014   574  1.257719  0.364739  0.306923  0.690638 -0.357792 -1.067481   \n",
      "1053   472 -3.043541 -3.157307  1.088463  2.288644  1.359805 -1.064823   \n",
      "1092   118  1.254914  0.350287  0.302488  0.693114 -0.371470 -1.070256   \n",
      "1131   574  1.257719  0.364739  0.306923  0.690638 -0.357792 -1.067481   \n",
      "1170   118  1.254914  0.350287  0.302488  0.693114 -0.371470 -1.070256   \n",
      "1209   529 -2.000567 -2.495484  2.467149  1.140053  2.462010  0.594262   \n",
      "1248   529 -2.000567 -2.495484  2.467149  1.140053  2.462010  0.594262   \n",
      "1287   574  1.257719  0.364739  0.306923  0.690638 -0.357792 -1.067481   \n",
      "1326     0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361   \n",
      "1365   472 -3.043541 -3.157307  1.088463  2.288644  1.359805 -1.064823   \n",
      "1404   118  1.254914  0.350287  0.302488  0.693114 -0.371470 -1.070256   \n",
      "1443   118  1.254914  0.350287  0.302488  0.693114 -0.371470 -1.070256   \n",
      "1482   574  1.257719  0.364739  0.306923  0.690638 -0.357792 -1.067481   \n",
      "1521   529 -2.000567 -2.495484  2.467149  1.140053  2.462010  0.594262   \n",
      "\n",
      "            V7        V8        V9  ...       V21       V22       V23  \\\n",
      "0     0.239599  0.098698  0.363787  ... -0.018307  0.277838 -0.110474   \n",
      "39   -0.031849  0.117620  0.017665  ... -0.037709  0.095701 -0.048198   \n",
      "78    1.061148 -0.410972 -0.179130  ...  0.003559  0.561240 -0.199287   \n",
      "117  -0.336555  0.670704  0.071670  ...  0.315868  0.847565  0.148877   \n",
      "156   0.656183  0.009755 -0.635963  ... -0.147934 -0.420046  0.061424   \n",
      "195  -0.061695 -0.125223  0.984938  ...  0.146077  0.481119 -0.140019   \n",
      "234  -0.428879  0.284565  0.406555  ...  0.162123  0.454364 -0.263576   \n",
      "273   0.736469 -0.327992 -0.555549  ...  0.009613  0.315739  0.054210   \n",
      "312  -0.279252 -0.205606 -0.647806  ...  0.071144  0.210635 -0.356278   \n",
      "351  -0.649554  0.534756  0.886183  ... -0.074659  0.397405  0.199030   \n",
      "390   0.238933  0.163402 -0.584981  ...  0.062165 -0.016076 -0.236314   \n",
      "429   1.315730 -0.010903 -0.360987  ...  0.154089  0.339031  0.372442   \n",
      "468  -0.086554  0.157807  1.677621  ...  0.361562 -0.173006  1.280446   \n",
      "507   0.531504 -0.081955 -0.522527  ... -0.070069  0.556788  0.217681   \n",
      "546  -1.057919  0.009986 -0.460486  ... -0.142455  0.042134 -0.125057   \n",
      "585  -0.793082  0.146590  1.676342  ...  0.220590  0.901360 -0.276484   \n",
      "624   0.055026  0.045165 -0.350573  ...  0.188378  0.487631 -0.147081   \n",
      "663   0.179124  0.616512 -0.151023  ...  0.008982 -0.075797 -0.057756   \n",
      "702   0.143747 -0.111374  0.790809  ...  0.081582  0.534693 -0.159644   \n",
      "741  -0.230310 -0.012302  0.123302  ...  0.171834  0.659725 -0.064483   \n",
      "780   0.325574 -0.067794 -0.270953  ...  0.661696  0.435477  1.375966   \n",
      "819  -2.537387  1.391657 -2.770089  ...  0.517232 -0.035049 -0.465211   \n",
      "858  -2.110183  0.788347  0.958809  ...  0.422452  1.195394  0.297836   \n",
      "897  -2.110183  0.788347  0.958809  ...  0.422452  1.195394  0.297836   \n",
      "936  -0.078803  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288   \n",
      "975   0.086781 -0.202836  0.035154  ... -0.287592 -0.832682  0.128083   \n",
      "1014  0.094272 -0.210300  0.014455  ... -0.286856 -0.820658  0.127663   \n",
      "1053  0.325574 -0.067794 -0.270953  ...  0.661696  0.435477  1.375966   \n",
      "1092  0.086781 -0.202836  0.035154  ... -0.287592 -0.832682  0.128083   \n",
      "1131  0.094272 -0.210300  0.014455  ... -0.286856 -0.820658  0.127663   \n",
      "1170  0.086781 -0.202836  0.035154  ... -0.287592 -0.832682  0.128083   \n",
      "1209 -2.110183  0.788347  0.958809  ...  0.422452  1.195394  0.297836   \n",
      "1248 -2.110183  0.788347  0.958809  ...  0.422452  1.195394  0.297836   \n",
      "1287  0.094272 -0.210300  0.014455  ... -0.286856 -0.820658  0.127663   \n",
      "1326 -0.078803  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288   \n",
      "1365  0.325574 -0.067794 -0.270953  ...  0.661696  0.435477  1.375966   \n",
      "1404  0.086781 -0.202836  0.035154  ... -0.287592 -0.832682  0.128083   \n",
      "1443  0.086781 -0.202836  0.035154  ... -0.287592 -0.832682  0.128083   \n",
      "1482  0.094272 -0.210300  0.014455  ... -0.286856 -0.820658  0.127663   \n",
      "1521 -2.110183  0.788347  0.958809  ...  0.422452  1.195394  0.297836   \n",
      "\n",
      "           V24       V25       V26       V27       V28  Amount  Class  \n",
      "0     0.066928  0.128539 -0.189115  0.133558 -0.021053  149.62      0  \n",
      "39    0.232115  0.606201 -0.342097  0.036770  0.007480    6.54      0  \n",
      "78    0.001387 -0.179530 -0.374116  0.071641 -0.175510    9.79      0  \n",
      "117   0.549791 -0.585131  0.325841 -0.068871  0.059713   50.00      0  \n",
      "156   0.520997 -0.238845  0.030135  0.140481  0.101163   14.98      0  \n",
      "195   0.538261  0.710720 -0.621382  0.036867  0.010963    8.80      0  \n",
      "234   0.144388  0.483056  0.222374  0.031597  0.056036  184.31      0  \n",
      "273   0.294232  0.003877 -0.314159 -0.099512  0.122697    1.00      0  \n",
      "312  -0.954151  0.963445  0.019195 -0.016289 -0.006384   34.00      0  \n",
      "351  -1.386013 -0.141955 -0.984011  0.274079 -0.019784   55.45      0  \n",
      "390  -0.082802  0.357494 -0.110530  0.080796  0.113264    1.00      0  \n",
      "429   0.172197 -0.230872 -0.363260 -0.067174 -0.066832  222.50      0  \n",
      "468   0.012697  0.760879 -0.828147 -0.298700 -0.061615  456.71      0  \n",
      "507   0.100721 -0.332479  0.252526  0.138865 -0.085152   29.18      0  \n",
      "546  -0.403785  0.212596  0.619155  0.035518  0.041399   72.00      0  \n",
      "585  -1.249997  0.646935 -0.440130  0.127178  0.027224    8.49      0  \n",
      "624   0.036691  0.565457 -0.275489  0.023386  0.019021   59.88      0  \n",
      "663  -0.388366 -0.296798  0.774677 -0.418797 -0.184978    3.00      0  \n",
      "702   0.296812  0.855793 -0.553063  0.054274  0.001410    1.00      0  \n",
      "741   0.125329  0.506081 -0.270854  0.069604  0.030001    1.00      0  \n",
      "780  -0.293803  0.279798 -0.145362 -0.252773  0.035764  529.00      1  \n",
      "819   0.320198  0.044519  0.177840  0.261145 -0.143276    0.00      1  \n",
      "858  -0.857105 -0.219322  0.861019 -0.124622 -0.171060    1.50      1  \n",
      "897  -0.857105 -0.219322  0.861019 -0.124622 -0.171060    1.50      1  \n",
      "936  -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69      1  \n",
      "975   0.339427  0.215944  0.094704 -0.023354  0.030892    2.69      1  \n",
      "1014  0.343128  0.221120  0.094391 -0.022189  0.030944    1.29      1  \n",
      "1053 -0.293803  0.279798 -0.145362 -0.252773  0.035764  529.00      1  \n",
      "1092  0.339427  0.215944  0.094704 -0.023354  0.030892    2.69      1  \n",
      "1131  0.343128  0.221120  0.094391 -0.022189  0.030944    1.29      1  \n",
      "1170  0.339427  0.215944  0.094704 -0.023354  0.030892    2.69      1  \n",
      "1209 -0.857105 -0.219322  0.861019 -0.124622 -0.171060    1.50      1  \n",
      "1248 -0.857105 -0.219322  0.861019 -0.124622 -0.171060    1.50      1  \n",
      "1287  0.343128  0.221120  0.094391 -0.022189  0.030944    1.29      1  \n",
      "1326 -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69      1  \n",
      "1365 -0.293803  0.279798 -0.145362 -0.252773  0.035764  529.00      1  \n",
      "1404  0.339427  0.215944  0.094704 -0.023354  0.030892    2.69      1  \n",
      "1443  0.339427  0.215944  0.094704 -0.023354  0.030892    2.69      1  \n",
      "1482  0.343128  0.221120  0.094391 -0.022189  0.030944    1.29      1  \n",
      "1521 -0.857105 -0.219322  0.861019 -0.124622 -0.171060    1.50      1  \n",
      "\n",
      "[40 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "n= int(math.sqrt(balanced_df.shape[0]))\n",
    "systematic_sample = balanced_df.iloc[::n]\n",
    "print(systematic_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression accuracy: 0.9285714285714286\n",
      "Decision Tree accuracy: 0.8571428571428571\n",
      "Support Vector Classification accuracy: 0.5714285714285714\n",
      "KNeighborsClassifier accuracy: 0.5\n",
      "Random Forest accuracy: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pulkitbhatia/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pulkitbhatia/opt/anaconda3/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Random_Sampling</th>\n",
       "      <th>Sytematic Sampling</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.895833</td>\n",
       "      <td>0.928571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree</th>\n",
       "      <td>0.979167</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC</th>\n",
       "      <td>0.697917</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>0.947917</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Random_Sampling  Sytematic Sampling\n",
       "Logistic Regression         0.895833            0.928571\n",
       "Decision Tree               0.979167            0.857143\n",
       "SVC                         0.697917            0.571429\n",
       "KNN                         0.947917            0.500000\n",
       "Random Forest               1.000000            1.000000"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "x_systematic= systematic_sample.drop(columns=['Class'])\n",
    "y_systematic = systematic_sample['Class']\n",
    "x_systematic_train, x_systematic_test,y_systematic_train, y_systematic_test = train_test_split(x_systematic,y_systematic, random_state=121, \n",
    "                                   test_size=0.33, \n",
    "                                   shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "# Train and evaluate the models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Support Vector Classification\": SVC(),\n",
    "    \"KNeighborsClassifier\":KNeighborsClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier()  \n",
    "}\n",
    "ans_systematic=[]\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(x_systematic_train,y_systematic_train)\n",
    "    y_pred = model.predict(x_systematic_test)\n",
    "    temp = accuracy_score(y_systematic_test, y_pred)\n",
    "    print(f\"{name} accuracy: {temp}\")\n",
    "    ans_systematic.append(temp)\n",
    "ans_df_systematic=pd.DataFrame(ans_systematic,index=['Logistic Regression','Decision Tree','SVC','KNN','Random Forest'])\n",
    "ans_df_systematic.rename(columns={ans_df_systematic.columns[0]: \"Random_Sampling\" }, inplace = True)\n",
    "ans_df['Sytematic Sampling']=ans_df_systematic\n",
    "ans_df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stratified Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = balanced_df.drop(columns=['Class'])\n",
    "y = balanced_df['Class']\n",
    "\n",
    "num_strata = 2\n",
    "samples = []\n",
    "\n",
    "for i in range(num_strata):\n",
    "    stratum_data = balanced_df[balanced_df['Class'] ==i]\n",
    "    stratum_size = len(stratum_data)\n",
    "    population_size=len(balanced_df)\n",
    "    p=0.5\n",
    "    error=0.05\n",
    "    z_score = 1.96  # for a 95% confidence level\n",
    "    p = stratum_size / population_size\n",
    "    q = 1 - p\n",
    "    n = int((z_score**2 * p * q * population_size) / ((z_score**2 * p * q) + (error**2 * (population_size-1))))\n",
    "\n",
    "    if n > stratum_size:\n",
    "        n = stratum_size\n",
    "\n",
    "    sample_indices = np.random.choice(stratum_data.index, size=n, replace=False)\n",
    "    stratum_sample = stratum_data.loc[sample_indices]\n",
    "    samples.append(stratum_sample)\n",
    "\n",
    "stratified_sample = pd.concat(samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression accuracy: 0.9211822660098522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pulkitbhatia/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pulkitbhatia/opt/anaconda3/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree accuracy: 0.9704433497536946\n",
      "Support Vector Classification accuracy: 0.6995073891625616\n",
      "KNeighborsClassifier accuracy: 0.9458128078817734\n",
      "Random Forest accuracy: 0.9950738916256158\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Random_Sampling</th>\n",
       "      <th>Sytematic Sampling</th>\n",
       "      <th>Stratified Sampling</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.895833</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.921182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree</th>\n",
       "      <td>0.979167</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.970443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC</th>\n",
       "      <td>0.697917</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.699507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>0.947917</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.945813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995074</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Random_Sampling  Sytematic Sampling  Stratified Sampling\n",
       "Logistic Regression         0.895833            0.928571             0.921182\n",
       "Decision Tree               0.979167            0.857143             0.970443\n",
       "SVC                         0.697917            0.571429             0.699507\n",
       "KNN                         0.947917            0.500000             0.945813\n",
       "Random Forest               1.000000            1.000000             0.995074"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "x_stratified= stratified_sample.drop(columns=['Class'])\n",
    "y_stratified = stratified_sample['Class']\n",
    "x_stratified_train, x_stratified_test,y_stratified_train, y_stratified_test = train_test_split(x_stratified,y_stratified, random_state=121, \n",
    "                                   test_size=0.33, \n",
    "                                   shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "# Train and evaluate the models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Support Vector Classification\": SVC(),\n",
    "    \"KNeighborsClassifier\":KNeighborsClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier()  \n",
    "}\n",
    "ans_stratified=[]\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(x_stratified_train,y_stratified_train)\n",
    "    y_pred = model.predict(x_stratified_test)\n",
    "    temp = accuracy_score(y_stratified_test, y_pred)\n",
    "    print(f\"{name} accuracy: {temp}\")\n",
    "    ans_stratified.append(temp)\n",
    "ans_df_stratified=pd.DataFrame(ans_stratified,index=['Logistic Regression','Decision Tree','SVC','KNN','Random Forest'])\n",
    "ans_df_stratified.rename(columns={ans_df_stratified.columns[0]: \"Random_Sampling\" }, inplace = True)\n",
    "ans_df['Stratified Sampling']=ans_df_stratified\n",
    "ans_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convinience Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "sample_size=100\n",
    "convinience_sample = balanced_df.sample(n=sample_size, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pulkitbhatia/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pulkitbhatia/opt/anaconda3/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression accuracy: 0.8484848484848485\n",
      "Decision Tree accuracy: 0.9696969696969697\n",
      "Support Vector Classification accuracy: 0.5454545454545454\n",
      "KNeighborsClassifier accuracy: 0.8181818181818182\n",
      "Random Forest accuracy: 1.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Random_Sampling</th>\n",
       "      <th>Sytematic Sampling</th>\n",
       "      <th>Stratified Sampling</th>\n",
       "      <th>Convinience Sampling</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.895833</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.921182</td>\n",
       "      <td>0.848485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree</th>\n",
       "      <td>0.979167</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.970443</td>\n",
       "      <td>0.969697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC</th>\n",
       "      <td>0.697917</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.699507</td>\n",
       "      <td>0.545455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>0.947917</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.945813</td>\n",
       "      <td>0.818182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995074</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Random_Sampling  Sytematic Sampling  Stratified Sampling  \\\n",
       "Logistic Regression         0.895833            0.928571             0.921182   \n",
       "Decision Tree               0.979167            0.857143             0.970443   \n",
       "SVC                         0.697917            0.571429             0.699507   \n",
       "KNN                         0.947917            0.500000             0.945813   \n",
       "Random Forest               1.000000            1.000000             0.995074   \n",
       "\n",
       "                     Convinience Sampling  \n",
       "Logistic Regression              0.848485  \n",
       "Decision Tree                    0.969697  \n",
       "SVC                              0.545455  \n",
       "KNN                              0.818182  \n",
       "Random Forest                    1.000000  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "x_convinience= convinience_sample.drop(columns=['Class'])\n",
    "y_convinience = convinience_sample['Class']\n",
    "x_convinience_train, x_convinience_test,y_convinience_train, y_convinience_test = train_test_split(x_convinience,y_convinience, random_state=121, \n",
    "                                   test_size=0.33, \n",
    "                                   shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "# Train and evaluate the models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Support Vector Classification\": SVC(),\n",
    "    \"KNeighborsClassifier\":KNeighborsClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier()  \n",
    "}\n",
    "ans_convinience=[]\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(x_convinience_train,y_convinience_train)\n",
    "    y_pred = model.predict(x_convinience_test)\n",
    "    temp = accuracy_score(y_convinience_test, y_pred)\n",
    "    print(f\"{name} accuracy: {temp}\")\n",
    "    ans_convinience.append(temp)\n",
    "ans_df_convinience=pd.DataFrame(ans_convinience,index=['Logistic Regression','Decision Tree','SVC','KNN','Random Forest'])\n",
    "ans_df_convinience.rename(columns={ans_df_convinience.columns[0]: \"Random_Sampling\" }, inplace = True)\n",
    "ans_df['Convinience Sampling']=ans_df_convinience\n",
    "ans_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "X = balanced_df.drop(columns=['Class'])\n",
    "y = balanced_df['Class']\n",
    "smote = SMOTE()\n",
    "\n",
    "x_smote, y_smote = smote.fit_resample(x, y)\n",
    "smote_random = pd.concat((pd.DataFrame(x_smote),pd.DataFrame(y_smote)),axis=1)\n",
    "np.random.seed(0)\n",
    "\n",
    "sample_size=int((pow(1.96,2)*0.5*0.5)/0.0025)\n",
    "smote_random_sample = smote_random.sample(n=sample_size, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pulkitbhatia/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pulkitbhatia/opt/anaconda3/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression accuracy: 0.8976377952755905\n",
      "Decision Tree accuracy: 0.9291338582677166\n",
      "Support Vector Classification accuracy: 0.6614173228346457\n",
      "KNeighborsClassifier accuracy: 0.9606299212598425\n",
      "Random Forest accuracy: 0.9921259842519685\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Random_Sampling</th>\n",
       "      <th>Sytematic Sampling</th>\n",
       "      <th>Stratified Sampling</th>\n",
       "      <th>Convinience Sampling</th>\n",
       "      <th>SMOTE Sampling</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.895833</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.921182</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.897638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree</th>\n",
       "      <td>0.979167</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.970443</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.929134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC</th>\n",
       "      <td>0.697917</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.699507</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.661417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>0.947917</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.945813</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.960630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995074</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.992126</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Random_Sampling  Sytematic Sampling  Stratified Sampling  \\\n",
       "Logistic Regression         0.895833            0.928571             0.921182   \n",
       "Decision Tree               0.979167            0.857143             0.970443   \n",
       "SVC                         0.697917            0.571429             0.699507   \n",
       "KNN                         0.947917            0.500000             0.945813   \n",
       "Random Forest               1.000000            1.000000             0.995074   \n",
       "\n",
       "                     Convinience Sampling  SMOTE Sampling  \n",
       "Logistic Regression              0.848485        0.897638  \n",
       "Decision Tree                    0.969697        0.929134  \n",
       "SVC                              0.545455        0.661417  \n",
       "KNN                              0.818182        0.960630  \n",
       "Random Forest                    1.000000        0.992126  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "x_SMOTE= smote_random_sample.drop(columns=['Class'])\n",
    "y_SMOTE = smote_random_sample['Class']\n",
    "x_SMOTE_train, x_SMOTE_test,y_SMOTE_train, y_SMOTE_test = train_test_split(x_SMOTE,y_SMOTE, random_state=121, \n",
    "                                   test_size=0.33, \n",
    "                                   shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "# Train and evaluate the models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Support Vector Classification\": SVC(),\n",
    "    \"KNeighborsClassifier\":KNeighborsClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier()  \n",
    "}\n",
    "ans_SMOTE=[]\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(x_SMOTE_train,y_SMOTE_train)\n",
    "    y_pred = model.predict(x_SMOTE_test)\n",
    "    temp = accuracy_score(y_SMOTE_test, y_pred)\n",
    "    print(f\"{name} accuracy: {temp}\")\n",
    "    ans_SMOTE.append(temp)\n",
    "ans_df_SMOTE=pd.DataFrame(ans_SMOTE,index=['Logistic Regression','Decision Tree','SVC','KNN','Random Forest'])\n",
    "ans_df_SMOTE.rename(columns={ans_df_SMOTE.columns[0]: \"Random_Sampling\" }, inplace = True)\n",
    "ans_df['SMOTE Sampling']=ans_df_SMOTE\n",
    "ans_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "286f8737588b73013c08b679e3233bd582a803f502937be7852962844806e4d4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
